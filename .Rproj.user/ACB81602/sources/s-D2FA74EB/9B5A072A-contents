---
title: "Introduction To Parallel Processing in R"
author: "Brittany Barker"
date: "`r format(Sys.Date(), tz = 'America/Los_Angeles')`"
output:
  html_document:
    df_print: paged
---
<style type="text/css">
body{ /* Normal  */
      font-size: 14px;
  }
p {line-height: 1.5em;}
</style>



## Parallel processing
- **What?** Parallel processing is a type of computation in which many calculations or the execution of processes are carried out simultaneously
- **Why?** Analyzing very large data sets and/or needing to run a computationally demanding analysis many times can be **very** slow
- **How?** Until the late 2000â€™s parallel computing was mainly done on clusters of large numbers of single- or dual-CPU computers. Nowadays even laptops have 2 or 4 CPU cores, and servers with 8, 32 or more cores are common.

## Operating system
- Your OS and number of CPU cores will affect the speed and capabilities of parallel processing 
- To check number of cores in Windows, go into "System Information" (shortcut = Windwows+R and type "msinfo32"), and click on "Processor"
- Multiple commands can check number of cores in Unix-based systems (e.g. Linux)
- In R, the `detectCores()` function of the `parallel` package returns the number of cores of your OS

```{r, warning = FALSE, message = FALSE}
library(parallel)

detectCores()
```

## Purpose of this demo
In this presentation, I will demonstrate the use and value of the `parallel`, `doParallel`, and `foreach` packages for running analyses in R in parallel.  The demo will center on analyzing very large spatial data sets, but parallel processing in R can be used for **ANY** type of data. We will see how changing the number of cores used for parallel processing affects run times, and how your OS (Windows vs. Unix-based OS) affects processing capabilities.


## Demo

The demo uses annual maximum temperature (Tmax) data from the [PRISM data base](https://prism.oregonstate.edu/). PRISM climate data are offered as daily, monthly, or annual rasters for the 48-state United States. Analyzing PRISM rasters can be very computationally demanding because of their fine spatial resolution (4 km), particularly for large regions.  We will conduct two different analyses that examine how Tmax has changed in recent years and compare run times with and with out parallel processing.  



### (1) County-level change in Tmax in July

**Question: For each year between 2016 and 2019, which OR counties experienced the hottest July temps compared to the 30 year average?**  

We will use three data sets: 

  1) Tmax for July for 2016, 2017, 2018 and 2019 (4 rasters)
  2) Average Tmax for July between 1981-2010 (1 raster)
  3) U.S. county simple features (sf) object  
  
```{r, warning = FALSE, message = FALSE}
library(dplyr)
library(raster)
library(USAboundaries)
library(sf)
library(stringr)

#install.packages("USAboundariesData", repos = "http://packages.ropensci.org", type = #"source")

# Load Tmax raster data from file
Jul_fls <- list.files("./prism_data/july/", 
                      pattern = glob2rx("*201*.bil$*"), full.names = TRUE)
Jul_30yr <- raster("./prism_data/normals/PRISM_tmax_30yr_normal_4kmM2_07_bil.bil") 

# Load county data, extract OR counties, and make into a SpatialPolygonsDataFrame
countiesOR <- us_counties() %>%
  mutate(state_county = paste(state_name, name, sep = "_")) %>%
  filter(state_name == "Oregon") %>%
  sf::as_Spatial(.)
```

Take a peek at the raster data - there are 872,505 raster cell values
```{r}
Jul_30yr

plot(Jul_30yr)
```

The `top5_func` function determines which 5 counties for any given year had the largest difference in July temperature compared to the 30 year average. 

```{r}
top5_func <- function(x) {
  
  # Subtract the 30 year average after converting both from DegC to DegF
  rast <- raster(x)
  anom_US <- (rast  * 1.8 + 32) - (Jul_30yr * 1.8 + 32)
  
  # Calculated difference (anomaly) in Tmax at each grid cell, and then 
  # calculate the average for each county
  anom_cnty <- extract(anom_US, countiesOR, fun = mean, df = TRUE)
  anom_cnty$county = countiesOR$name
  
  # Sort the results and extract the 5 counties with the largest anomalies
  anom_cnty <- anom_cnty %>% 
    mutate(difference_F = layer,
           difference_F_abs = abs(difference_F),
           county = countiesOR$name) %>%
    arrange(desc(difference_F_abs)) %>%
    # filter( min_rank(desc(difference_F)) <= 5 | 
    #                 min_rank(difference_F) <= 5 ) 
    top_n(5) %>%
    dplyr::select(county, difference_F)
}
```

### Analyze each year sequentially

The list of Tmax rasters for each year may be analyzed sequentially using `lapply` or a `for` loop.
```{r, warning = FALSE, message = FALSE}
# Apply the function to the list of Tmax rasters
top5_sequential <- system.time(
  top5.1 <- lapply(Jul_fls, top5_func)
)

top5_sequential
```

### Analyze years in parallel

Running the analysis sequentially is pretty slow. Instead, Tmax data for each year can be analyzed in parallel. 

First, we need to create a parallel socket cluster (another word for core) using `parallel::makeCluster`, which creates a user-specified number of copies of R to run in parallel. WARNING: specifying too many cores may overload your computer and potentially crash processes and programs (including R). 

Since only 4 Tmax data sets are being analyzed, it is pointless to specify more than 4 cores. Let's compare run times using 2 vs. 4 cores.

```{r}
cl.2 <- makeCluster(2)
cl.4 <- makeCluster(4)
```

At least two approaches may be used to run the analysis in parallel.    

#### **Approach 1:** foreach 
A `foreach::foreach` loop is essentially a hybrid of the standard `for` loop and the `lapply` function, and provides a looping construct for executing R code repeatedly in parallel. The `doParallel` package provides the backend to execute the `foreach` loop.  

Comparing a `foreach` loop to a `for` loop  
- The `%dopar%` function exports the tasks to parallel execution workers  
- A `foreach` loop returns a list of results by default (the `.combine` option can be used to specify how results are returned)  
- The R packages needed to complete the work must be specified in a `foreach` loop  
- Objects in a `foreach` loop are not in the global environment unless you export them  

```{r, warning = FALSE, message = FALSE}
library(foreach)
library(doParallel)

# Analysis using 2 cores
registerDoParallel(cl.2) # Register the parallel backend
top5_foreach_cl2 <- system.time(
  # The required R packages must be specified in a foreach loop
  top5.2 <- foreach(fl = Jul_fls, .packages = c("raster", "dplyr"))
  %dopar% { 
    top5_func(fl)
  }
)

# Run time - 2 cores
top5_foreach_cl2
```

You should stop the clusters after you're done.

```{r}
stopCluster(cl.2)
```

````{r}
# Analysis using 4 cores
registerDoParallel(cl.4)
top5_foreach_cl4 <- system.time(
  top5.3 <- foreach(fl = Jul_fls, .packages = c("raster", "dplyr"))
  %dopar% { 
    top5_func(fl)
  }
)

# Run time - 4 cores
top5_foreach_cl4

stopCluster(cl.4) # Stop cluster
```


#### **Approach 2**: mclapply
The `parallel::mclapply` function is analogous to `lapply`, but it distributes the tasks to multiple processors.  

Unfortunately, `mclapply` does not work on Windows machines because its implementation relies on forking and Windows does not support forking. "Fork" is the name of the Unix call that the parent process uses to "divide" itself ("fork") into two identical processes that run independently from each other. Windows uses a threading module for multiprocessing, meaning that the process shares memory and resources.

By default, `mclapply` will use all cores available to it. You can specify the number of cores to use with the `mc.cores` option.

```{r}
# Analysis using 2 cores
top5_mclapply_cl2 <- system.time(
  top5.4 <- mclapply(Jul_fls, top5_func, mc.cores = 2)
)

# Run time - 2 cores
top5_mclapply_cl2
```
```{r}
# Analysis using 4 cores
top5_mclapply_cl4 <- system.time(
  top5.5 <- mclapply(Jul_fls, top5_func, mc.cores = 4)
)

# Run time - 4 cores
top5_mclapply_cl4
```

### Compare run times for top 5 county analysis
Notice how parallel processing using 4 cores is twice as fast as sequential processing. Run times for the analysis conducted in a `foreach` loop vs. with `mclapply` are very similar, although `mclapply` may be marginally faster.

```{r}
# Compare run times
all_times1 <- data.frame(rbind(top5_sequential, top5_foreach_cl2, 
                               top5_foreach_cl4, top5_mclapply_cl2, 
                               top5_mclapply_cl4)) %>%
  tibble::rownames_to_column(var = "run") %>%
  dplyr::select(run, elapsed)

knitr::kable(all_times1)
```

### Top 5 county results and takeaway
The output below shows which 5 OR counties had the largest difference (anomaly) in temperature (F) from the 30 year average for July. It looks like July max temps across OR counties in 2016 and 2019 were in fact cooler than the 30 year average. Results for each year were generated much quicker by running July Tmax data for each year in parallel.

```{r}
names(top5.1) <- as.character(2016:2019)
top5.1 
```


### (2) Changes in annual maximum temperatures in Oregon between 1990 and 2019  

**Question: What was the trend in annual Tmax across Oregon for each decade between 1990 and 2019?**    

### Load, crop, and subset annual Tmax rasters

```{r}
# Load and crop Tmax data for entire U.S. for all years between 1990 and 2019
USA <- stack(list.files(path = paste0(getwd(), "/prism_data/annual/"),
                        pattern = ".bil$*", full.names = TRUE)) # A stack of rasters
OR <- crop(USA, extent(-124.7294, -116.2949, 41.7150, 46.4612)) # Crop the raster stack

# Separate rasters by decade 
OR.90_99 <- OR[[1:10]]
OR.00_10 <- OR[[11:21]] 
OR.11_19 <- OR[[22:30]]
OR_sets <- list(OR.90_99, OR.00_10, OR.11_19)
```


### Slope function

The `slope_func` function below calculates the slope at each raster grid cell, which provides insight into the direction of change in annual Tmax for each decade.

```{r}
slope_func <- function(r) {
  years <- str_split_fixed(names(r), "_", 6)[,5]
  if (all(is.na(r))) {
    NA
  } else {
    m <- lm(r ~ years); summary(m)$coefficients[2] # Calculates the slope
  }
}
```


### Analyze trend of annual Tmax for each decade sequentially

```{r}
trend_sequential <- system.time(
  slopes <- lapply(OR_sets, function(x) {
    calc(x, slope_func)
  })
)

trend_sequential
```


### Analyze trend of annual Tmax for decades in parallel

#### **Approach 1**: foreach
Using a `foreach` loop with 3 cores, since the analysis is being conducted on 3 data sets.

```{r}
cl.3 <- makeCluster(3)
registerDoParallel(cl.3)

trend_foreach <- system.time(
  slopes <- foreach(set = OR_sets, 
                    .packages = c("raster", "stringr")) %dopar% { 
    calc(set, slope_func)
  }
)

trend_foreach

stopCluster(cl.3)
```


#### **Approach 2**: mclapply
Using `mclapply`

```{r}
trend_mclapply <- system.time(
  slopes <- mclapply(OR_sets, function(x) {
    calc(x, slope_func)
  }, mc.cores = 3)
)

trend_mclapply
```

### Tmax trend results and takeaway
Again, we see marked run times improvements in using parallel processing. Parallel processing (3 cores) was ~3 times faster than sequential processing. 

```{r}
# Compare run times
all_times2 <- data.frame(rbind(trend_sequential, 
                               trend_foreach, trend_mclapply)) %>%
  tibble::rownames_to_column(var = "run") %>%
  dplyr::select(run, elapsed)

knitr::kable(all_times2)
```

It looks like annual Tmax exhibited different trends throughout OR if we look at each decade separately, but overall it seems like increases have been higher in the interior parts of the state. Results for each year were generated much quicker by calculating trends for each year in parallel.

```{r}
# Look at the trend results
names(slopes[[1]]) <- c("OR_1990_1999")
names(slopes[[2]]) <- c("OR_2000_2010")
names(slopes[[3]]) <- c("OR_2011_2019")

plot(stack(slopes[[1]], slopes[[2]], slopes[[3]]))
```

### Wrap-up
Parallel processing in R can significantly reduce run times   
- In this demo, run times were cut in half or two thirds depending on the analysis   
- Time savings between sequential vs. parallel computations would be even greater if we had analyzed even more data sets in parallel  
- For example, let's say we had to analyze changes in Tmax in July across all 3,006 U.S. counties!  
- Access to servers with numerous cores (24, 48, etc.) may be essential 

Consider using parallel processing in R when  
- You need to implement repetitive tasks  
- There is no other way to speed up computations  
- You have access to enough cores to make a notable difference in speed    

It might not be worth it when  
- It's overloading your computer and causing programs to stall or crash  
- This may happen if your data set is extremely large, such that using even 2 or 3 cores is too much  
- You're able to speed up an analysis by other means (e.g. a more efficient function)  


### Potentially useful vignettes, demos, and introductions
- The `multidplyr` [package](https://rdrr.io/github/hadley/multidplyr/) partitions a data frame across multiple worker processes to provide simple multicore parallelism  
- A `foreach` [vignette](https://cran.r-project.org/web/packages/foreach/vignettes/foreach.html)   
- R-bloggers demo [Let's be Faster and more Parallel in R with doParallel package](https://www.r-bloggers.com/lets-be-faster-and-more-parallel-in-r-with-doparallel-package/)  
- [How to go parallel in R - basics + tips](http://gforge.se/2015/02/how-to-go-parallel-in-r-basics-tips/#The_foreach_package) with emphasis on `foreach()`  
- Introduction to [Parallel computing in R](https://psu-psychology.github.io/r-bootcamp-2018/talks/parallel_r.html)  
- [Quick Intro to Parallel Computing in R](https://nceas.github.io/oss-lessons/parallel-computing-in-r/parallel-computing-in-r.html) using remote sensing data  
- [Efficient Looping with R](http://ethen8181.github.io/Business-Analytics/R/efficient_looping/efficient_looping.html)  
- [Examples of functions in the parallel package](https://www.r-bloggers.com/simple-parallel-processing-in-r)